{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72e4fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saptarshimallikthakur/Desktop/tracking/.venv/lib/python3.10/site-packages/torchreid/reid/metrics/rank.py:11: UserWarning: Cython evaluation (very fast so highly recommended) is unavailable, now use python evaluation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchreid import models\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71d90733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Users/saptarshimallikthakur/Desktop/tracking/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Users/saptarshimallikthakur/Desktop/tracking/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Users/saptarshimallikthakur/Desktop/tracking/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Users/saptarshimallikthakur/Desktop/tracking/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Users/saptarshimallikthakur/Desktop/tracking/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>/Users/saptarshimallikthakur/Desktop/tracking/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>/Users/saptarshimallikthakur/Desktop/tracking/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1831</th>\n",
       "      <td>/Users/saptarshimallikthakur/Desktop/tracking/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1832</th>\n",
       "      <td>/Users/saptarshimallikthakur/Desktop/tracking/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1833</th>\n",
       "      <td>/Users/saptarshimallikthakur/Desktop/tracking/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1834 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               img_path  label\n",
       "0     /Users/saptarshimallikthakur/Desktop/tracking/...      1\n",
       "1     /Users/saptarshimallikthakur/Desktop/tracking/...      1\n",
       "2     /Users/saptarshimallikthakur/Desktop/tracking/...      1\n",
       "3     /Users/saptarshimallikthakur/Desktop/tracking/...      1\n",
       "4     /Users/saptarshimallikthakur/Desktop/tracking/...      1\n",
       "...                                                 ...    ...\n",
       "1829  /Users/saptarshimallikthakur/Desktop/tracking/...      0\n",
       "1830  /Users/saptarshimallikthakur/Desktop/tracking/...      0\n",
       "1831  /Users/saptarshimallikthakur/Desktop/tracking/...      0\n",
       "1832  /Users/saptarshimallikthakur/Desktop/tracking/...      0\n",
       "1833  /Users/saptarshimallikthakur/Desktop/tracking/...      0\n",
       "\n",
       "[1834 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_image_dataframe(base_path):\n",
    "    data = []\n",
    "\n",
    "    # Define label mapping\n",
    "    label_map = {\n",
    "        'employees': 1,\n",
    "        'customers': 0\n",
    "    }\n",
    "\n",
    "    for category in ['employees', 'customers']:\n",
    "        category_path = os.path.join(base_path, category)\n",
    "        label = label_map[category]\n",
    "\n",
    "        for subfolder in os.listdir(category_path):\n",
    "            subfolder_path = os.path.join(category_path, subfolder)\n",
    "            if not os.path.isdir(subfolder_path):\n",
    "                continue\n",
    "\n",
    "            for filename in os.listdir(subfolder_path):\n",
    "                if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                    img_path = os.path.join(subfolder_path, filename)\n",
    "                    data.append({'img_path': img_path, 'label': label})\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Example usage\n",
    "# Replace 'data/' with your actual base directory path\n",
    "df = create_image_dataframe('/Users/saptarshimallikthakur/Desktop/tracking/Bluetokai/IN OUT/data')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0180d423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    1253\n",
       "1     581\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36fe553e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Build & load the Market1501-pretrained OSNet ×1.0\n",
    "def build_reid_model(checkpoint_path, device='cpu'):\n",
    "    \n",
    "    # a) instantiate the architecture\n",
    "    model = models.osnet_x1_0(\n",
    "        num_classes=1041,\n",
    "        loss='softmax',\n",
    "        pretrained=False,\n",
    "        use_pretrained_backbone=False\n",
    "    )\n",
    "\n",
    "    # b) load the checkpoint\n",
    "    ckpt = torch.load(checkpoint_path, map_location='cpu')\n",
    "    state_dict = ckpt.get('state_dict', ckpt)\n",
    "\n",
    "    # c) strip \"module.\" if present\n",
    "    clean_state = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(clean_state)\n",
    "    return model.to(device).eval()\n",
    "\n",
    "# 2) Standard ReID preprocessing (256×128 + ImageNet norm)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((256, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std =[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def extract_embeddings_both(model, img_path, device='cpu'):\n",
    "#     pil_img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "#     # Original\n",
    "#     x_orig = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "#     f_orig = model(x_orig).squeeze(0).cpu().numpy()\n",
    "\n",
    "#     # Flipped\n",
    "#     pil_flip = transforms.functional.hflip(pil_img)\n",
    "#     x_flip = preprocess(pil_flip).unsqueeze(0).to(device)\n",
    "#     f_flip = model(x_flip).squeeze(0).cpu().numpy()\n",
    "\n",
    "#     return f_orig, f_flip\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_embeddings_both(model, img_path, device='cpu'):\n",
    "    pil_img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "    # Original\n",
    "    x_orig = preprocess(pil_img).unsqueeze(0).to(device)\n",
    "    f_orig = model(x_orig).squeeze(0).cpu().numpy()\n",
    "\n",
    "    return f_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27284859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1834/1834 [00:40<00:00, 44.84it/s]\n"
     ]
    }
   ],
   "source": [
    "# # 1) load & freeze your 512-D extractor\n",
    "# backbone = build_reid_model(\"osnet_x1_0_msmt17_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth\", device='mps')\n",
    "# backbone.eval()\n",
    "\n",
    "# # 2) precompute all embeddings\n",
    "# embs, labs = [], []\n",
    "# for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#     f1, f2 = extract_embeddings_both(backbone, row.img_path, device='mps')\n",
    "    \n",
    "#     embs.extend([f1, f2])              # add both embeddings\n",
    "#     labs.extend([int(row.label)] * 2)  # same label for both\n",
    "\n",
    "# embs = torch.from_numpy(np.stack(embs)).float()  # [N,512]\n",
    "# labs = torch.tensor(labs).long()                 # [N]\n",
    "\n",
    "\n",
    "# 1) load & freeze your 512-D extractor\n",
    "backbone = build_reid_model(\"osnet_x1_0_msmt17_256x128_amsgrad_ep150_stp60_lr0.0015_b64_fb10_softmax_labelsmooth_flip.pth\", device='mps')\n",
    "backbone.eval()\n",
    "\n",
    "# 2) precompute all embeddings\n",
    "embs, labs = [], []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    f1 = extract_embeddings_both(backbone, row.img_path, device='mps')\n",
    "    \n",
    "    embs.append(f1)            # add both embeddings\n",
    "    labs.append(row.label) # same label for both\n",
    "\n",
    "embs = torch.from_numpy(np.stack(embs)).float()  # [N,512]\n",
    "labs = torch.tensor(labs).long()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f7810c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1834, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "205e72bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) make train/val splits\n",
    "dataset = TensorDataset(embs, labs)\n",
    "val_sz = int(len(dataset)*0.1)\n",
    "train_ds, val_ds = random_split(dataset, [len(dataset)-val_sz, val_sz])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=8)\n",
    "\n",
    "# 4) define your head\n",
    "hidden_dim = 16\n",
    "head = nn.Sequential(\n",
    "    nn.Linear(512, hidden_dim),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(hidden_dim, 2)\n",
    ").to('mps')\n",
    "\n",
    "opt = optim.Adam(head.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "crit = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96002861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01  Val Acc: 0.8852  (best: 0.8852)\n",
      "Epoch 02  Val Acc: 0.9071  (best: 0.9071)\n",
      "Epoch 03  Val Acc: 0.9126  (best: 0.9126)\n",
      "Epoch 04  Val Acc: 0.9071  (best: 0.9126)\n",
      "Epoch 05  Val Acc: 0.9071  (best: 0.9126)\n",
      "Epoch 06  Val Acc: 0.9071  (best: 0.9126)\n",
      "Epoch 07  Val Acc: 0.9126  (best: 0.9126)\n",
      "Early stopping after 8 epochs (no improvement in 5 epochs)\n",
      "Best val acc: 0.912568306010929\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-params for early stopping\n",
    "patience = 5           # how many epochs to wait after last improvement\n",
    "best_acc = 0.0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, 51):\n",
    "    head.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to('mps'), y.to('mps')\n",
    "        opt.zero_grad()\n",
    "        loss = crit(head(x), y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    # evaluation\n",
    "    head.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to('mps'), y.to('mps')\n",
    "            preds = head(x).argmax(1)\n",
    "            correct += (preds == y).sum().item()\n",
    "    acc = correct / len(val_ds)\n",
    "\n",
    "    # check for improvement\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        epochs_no_improve = 0\n",
    "        # (optional) save the best model\n",
    "        torch.save(head.state_dict(), \"best_head.pt\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping after {epoch} epochs (no improvement in {patience} epochs)\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}  Val Acc: {acc:.4f}  (best: {best_acc:.4f})\")\n",
    "\n",
    "print(\"Best val acc:\", best_acc)\n",
    "\n",
    "head.load_state_dict(torch.load(\"best_head.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34410215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ exported → employee_classifier.onnx\n"
     ]
    }
   ],
   "source": [
    "class EmployeeClassifier(nn.Module):\n",
    "    def __init__(self, backbone, head):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone.eval()     # 512-D features\n",
    "        self.head     = head.eval()         # 2-class logits\n",
    "\n",
    "    def forward(self, x):                   # x: [B,3,256,128]\n",
    "        feats = self.backbone(x)            # [B,512]\n",
    "        return self.head(feats)             # [B,2]\n",
    "    \n",
    "full_model = EmployeeClassifier(backbone, head).cpu()   # ONNX export works on CPU\n",
    "dummy      = torch.randn(1, 3, 256, 128)                # BCHW\n",
    "\n",
    "torch.onnx.export(\n",
    "    full_model,                            # model\n",
    "    dummy,                                 # example input\n",
    "    \"employee_classifier.onnx\",            # file to write\n",
    "    opset_version = 12,                    # 11+ works with ONNX-Runtime\n",
    "    input_names  = [\"images\"],\n",
    "    output_names = [\"logits\"],\n",
    "    dynamic_axes = {\"images\": {0: \"batch\"}, \"logits\": {0: \"batch\"}}\n",
    ")\n",
    "print(\"✓ exported → employee_classifier.onnx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8922f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "model = onnx.load(\"employee_classifier.onnx\")\n",
    "onnx.checker.check_model(model)      # throws if something is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37ed3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Imagenet means/std you used\n",
    "IMNET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "IMNET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "def preprocess(img_path:str) -> np.ndarray:\n",
    "    img = cv2.imread(img_path)                 # BGR uint8\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (128, 256))          # (W,H)\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = (img - IMNET_MEAN) / IMNET_STD\n",
    "    img = np.transpose(img, (2,0,1))           # CHW\n",
    "    return np.expand_dims(img, 0)              # [1,3,256,128]\n",
    "\n",
    "sess = ort.InferenceSession(\"employee_classifier.onnx\",\n",
    "                            providers=[\"CPUExecutionProvider\"])   # or CUDAExecutionProvider\n",
    "\n",
    "def predict(img_path:str) -> int:\n",
    "    inp  = preprocess(img_path)                         # float32\n",
    "    logits = sess.run(None, {\"images\": inp})[0]         # [1,2]\n",
    "    return int(logits.argmax(1)[0])                     # 1 = employee, 0 = customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a6fb43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
