{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4e04cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-05-16 11:19:49.671826 [W:onnxruntime:, coreml_execution_provider.cc:112 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 5 number of nodes in the graph: 320 number of nodes supported by CoreML: 315\u001b[m\n",
      "\u001b[0;93m2025-05-16 11:19:51.060373 [W:onnxruntime:, session_state.cc:1263 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-16 11:19:51.060380 [W:onnxruntime:, session_state.cc:1265 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "\u001b[0;93m2025-05-16 11:19:51.073744 [W:onnxruntime:, coreml_execution_provider.cc:112 GetCapability] CoreMLExecutionProvider::GetCapability, number of partitions supported by CoreML: 3 number of nodes in the graph: 394 number of nodes supported by CoreML: 392\u001b[m\n",
      "\u001b[0;93m2025-05-16 11:19:51.899663 [W:onnxruntime:, session_state.cc:1263 VerifyEachNodeIsAssignedToAnEp] Some nodes were not assigned to the preferred execution providers which may or may not have an negative impact on performance. e.g. ORT explicitly assigns shape related ops to CPU to improve perf.\u001b[m\n",
      "\u001b[0;93m2025-05-16 11:19:51.899672 [W:onnxruntime:, session_state.cc:1265 VerifyEachNodeIsAssignedToAnEp] Rerunning with verbose output on a non-minimal build will show node assignments.\u001b[m\n",
      "2025-05-16 11:19:52.348 Python[8296:146682] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-05-16 11:19:52.348 Python[8296:146682] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "import config as cfg\n",
    "from detection import preprocess_image, process_output\n",
    "from classification import predict_from_crop\n",
    "from tracker_module import ObjectTracker\n",
    "\n",
    "# --- setup sessions, video, tracker, etc. ---\n",
    "detection_session = ort.InferenceSession(cfg.detection_model_path, providers=cfg.providers)\n",
    "input_name = detection_session.get_inputs()[0].name\n",
    "output_names = [o.name for o in detection_session.get_outputs()]\n",
    "iw, ih = detection_session.get_inputs()[0].shape[2:]\n",
    "\n",
    "classification_session = ort.InferenceSession(cfg.classification_model_path, providers=cfg.providers)\n",
    "\n",
    "cap = cv2.VideoCapture(cfg.video_path)\n",
    "w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Define codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  \n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "out = cv2.VideoWriter('customer classification IN OUT.mp4', fourcc, fps, (w, h))\n",
    "\n",
    "diag = np.hypot(w, h)\n",
    "tracker = ObjectTracker(iou_threshold=0.3, max_dist=0.2 * diag, expire_after=24, BASE_IOU_W=0.8, BASE_DIR_W=0.2)\n",
    "\n",
    "# --- counting infrastructure ---\n",
    "track_labels = defaultdict(list)    # tid -> [labels...]\n",
    "track_prev_cx = {}                  # tid -> last cx\n",
    "counted_ids = set()                 # tids already counted\n",
    "IN_count = 0\n",
    "\n",
    "start_line_x = int(w//8)\n",
    "start_line_y1 = int(1*h//5)\n",
    "start_line_y2 = int(4*h // 5)\n",
    "IN_line    = int(w // 2)\n",
    "\n",
    "frame_count = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "    if frame_count % 2 != 0:\n",
    "        continue\n",
    "\n",
    "    # ── detection ─────────────────────────────────────────────────\n",
    "    blob, ratio, orig_shape = preprocess_image(frame, (iw, ih))\n",
    "    outs = detection_session.run(output_names, {input_name: blob})\n",
    "    dets = process_output(outs, conf_threshold=0.4, nms_threshold=0.5, img_shape=orig_shape, ratio=ratio)\n",
    "\n",
    "    # ── tracking ──────────────────────────────────────────────────\n",
    "    active = tracker.update([d['box'] for d in dets], frame_count)\n",
    "\n",
    "    for tid, box in active.items():\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        cx,cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "        # skip if still left of the line OR cy is outside [y1, y2]\n",
    "        if cx < start_line_x or cy < start_line_y1 or cy > start_line_y2:\n",
    "            track_prev_cx[tid] = cx\n",
    "            continue\n",
    "\n",
    "\n",
    "        # crop & classify\n",
    "        crop = frame[y1:y2, x1:x2]\n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "        label = predict_from_crop(classification_session, crop)\n",
    "        track_labels[tid].append(label)\n",
    "\n",
    "        # draw box + label + ID\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0,255,0), 2)\n",
    "        cv2.putText(frame, f\"{label}\", (x1, y1-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "        cv2.putText(frame, f\"ID {tid}\", (x2-15, y1-8),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0,255,0), 2)\n",
    "\n",
    "        # check crossing left→right over IN_line\n",
    "        prev_cx = track_prev_cx.get(tid)\n",
    "        if prev_cx is not None and prev_cx < IN_line <= cx and tid not in counted_ids:\n",
    "            # get mode label\n",
    "            mode_label = Counter(track_labels[tid]).most_common(1)[0][0]\n",
    "            if mode_label == 'customer':\n",
    "                IN_count += 1\n",
    "            counted_ids.add(tid)\n",
    "\n",
    "        # update prev position\n",
    "        track_prev_cx[tid] = cx\n",
    "\n",
    "    # Vertical red line (between y1 and y2)\n",
    "    cv2.line(frame, (start_line_x, start_line_y1), (start_line_x, start_line_y2), (0, 0, 255), 2)\n",
    "\n",
    "    # Horizontal top red line from start_line_x to right edge\n",
    "    cv2.line(frame, (start_line_x, start_line_y1), (IN_line, start_line_y1), (0, 0, 255), 2)\n",
    "\n",
    "    # Horizontal bottom red line from start_line_x to right edge\n",
    "    cv2.line(frame, (start_line_x, start_line_y2), (IN_line, start_line_y2), (0, 0, 255), 2)\n",
    "\n",
    "    # ── IN LINE (Green, full height) ─────────\n",
    "    cv2.line(frame, (IN_line, start_line_y1), (IN_line, start_line_y2), (0, 255, 0), 3)\n",
    "\n",
    "\n",
    "    cv2.putText(frame, f\"IN: {IN_count}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow('People Detection with Count', frame)\n",
    "    out.write(frame)  # Write the processed frame to output file\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9550ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
